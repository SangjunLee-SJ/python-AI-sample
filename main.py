# python-AI-sample
# THIS CODE IS Generated BY ChatGPT and Copliot
## But not like the simple python code, this code takes lots of updates(by me). So, it's not AI-only-generated code.
## Still under development

# Using the ISIC dataset(https://www.isic-archive.com/)

import torch
import torchvision
from torchvision import transforms
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader
import pandas as pd

import torch.nn as nn
import torch.optim as optim

data_dir = "./skinaitest/skinaitest/images"

class ISICDataset(Dataset):
    def __init__(self, data_dir, metadata_path, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.images = []
        self.labels = []
        
        # Read the metadata file
        metadata = pd.read_csv(metadata_path, low_memory=False)
        # Extract the file names and labels
        file_names = metadata['isic_id'].tolist()
        labels = metadata['benign_malignant'].tolist()
        
        for i, file_name in enumerate(file_names):
            # Add the file extension to the file name
            file_name += '.JPG'
            image_path = os.path.join(data_dir, file_name)
            # Open the image file
            image = Image.open(image_path)
            # Convert the image to a tensor
            image = transforms.ToTensor()(image)
            self.images.append(image)
            self.labels.append(labels[i])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

def preprocess(image):
    # Perform any preprocessing steps on the image here
    return image

metadata_path = "./skinaitest/skinaitest/images/metadata.csv"
dataset = ISICDataset(data_dir, metadata_path, transform=preprocess)

# Create the CNN model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc

# Instantiate the model, loss function and optimizer
net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

# Define the number of epochs and the dataloader
num_epochs = 10
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Start the training loop
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in enumerate(dataloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f"Epoch {epoch + 1} loss: {running_loss / len(dataloader)}")
print("Finished Training")